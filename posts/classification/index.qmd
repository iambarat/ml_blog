---
title: "Understanding Classification"
author: "Mohaimin"
date: "2023-12-03"
format: html
categories: [classification]
---

# Introduction to Classification in Machine Learning

Classification is a cornerstone of machine learning, where we teach a computer how to make decisions from data. It involves categorizing data into predefined classes. In this post, we'll explore the basics of classification in machine learning using Python, a popular language in data science due to its simplicity and robust libraries.


## Classification Basics

Classification algorithms represent a fundamental aspect of supervised learning in machine learning. These algorithms focus on categorizing new observations into predefined classes or groups based on a dataset of labeled examples. In simpler terms, classification involves using these algorithms to predict the category or class of new, unseen instances.

The essence of classification is to "learn" from a dataset where the categories (or labels) of observations are known, and then apply this learned pattern to predict the class of new observations. Common examples include binary classifications like Yes or No, 0 or 1, Spam or Not Spam, and multi-class scenarios like distinguishing between different types of animals, colors, or objects. In contrast to regression, where the output is a continuous value, the output of a classification algorithm is categorical. These categories, also known as labels or targets, are discrete and typically non-numeric. For instance, categories might be "Red" or "Blue", "Cat" or "Dog", "Spam" or "Not Spam". 

As a supervised learning technique, classification relies on labeled input data. This means that the training data includes both the input features and the corresponding output labels. The algorithm learns to associate specific input patterns with particular output categories, enabling it to make predictions for new, unlabeled data.


### Types of Classification

**Binary Classification:** Involves two classes. For example, determining whether an email is spam. 

**Multiclass Classification:** Involves more than two classes. For instance, classifying types of crops.

### Implementing Classification in Python

We'll use the Iris dataset, a classic in machine learning, which includes data on various iris flowers and classifies them into three species.

<!--
```{r}
library(reticulate)
use_python("/Users/bota/Files/VT/Course Materials/Fall 23/ML Blog/vscode/mlblog/bin/python")
```
-->

```{python}
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# Load Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# For simplicity, using only two classes
X, y = X[y != 2], y[y != 2]

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Logistic Regression Classifier
clf_lr = LogisticRegression()
clf_lr.fit(X_train, y_train)
y_pred_lr = clf_lr.predict(X_test)
cm_lr = confusion_matrix(y_test, y_pred_lr)
accuracy_lr = accuracy_score(y_test, y_pred_lr)

# Decision Tree Classifier
clf_dt = DecisionTreeClassifier()
clf_dt.fit(X_train, y_train)
y_pred_dt = clf_dt.predict(X_test)
cm_dt = confusion_matrix(y_test, y_pred_dt)
accuracy_dt = accuracy_score(y_test, y_pred_dt)

# SVM Classifier
clf_svm = SVC(probability=True)
clf_svm.fit(X_train, y_train)
y_pred_svm = clf_svm.predict(X_test)
cm_svm = confusion_matrix(y_test, y_pred_svm)
accuracy_svm = accuracy_score(y_test, y_pred_svm)

# KNN Classifier
clf_knn = KNeighborsClassifier(n_neighbors=3)
clf_knn.fit(X_train, y_train)
y_pred_knn = clf_knn.predict(X_test)
cm_knn = confusion_matrix(y_test, y_pred_knn)
accuracy_knn = accuracy_score(y_test, y_pred_knn)

# Plotting Confusion Matrices for all classifiers
fig, axes = plt.subplots(2, 2, figsize=(12, 12))

# Logistic Regression
sns.heatmap(cm_lr, annot=True, fmt="d", cmap="Blues", ax=axes[0, 0])
axes[0, 0].set_title('Logistic Regression Confusion Matrix')
axes[0, 0].set_xlabel('Predicted Label')
axes[0, 0].set_ylabel('True Label')

# Decision Tree
sns.heatmap(cm_dt, annot=True, fmt="d", cmap="Blues", ax=axes[0, 1])
axes[0, 1].set_title('Decision Tree Confusion Matrix')
axes[0, 1].set_xlabel('Predicted Label')
axes[0, 1].set_ylabel('True Label')

# SVM
sns.heatmap(cm_svm, annot=True, fmt="d", cmap="Blues", ax=axes[1, 0])
axes[1, 0].set_title('SVM Confusion Matrix')
axes[1, 0].set_xlabel('Predicted Label')
axes[1, 0].set_ylabel('True Label')

# KNN
sns.heatmap(cm_knn, annot=True, fmt="d", cmap="Blues", ax=axes[1, 1])
axes[1, 1].set_title('KNN Confusion Matrix')
axes[1, 1].set_xlabel('Predicted Label')
axes[1, 1].set_ylabel('True Label')

plt.tight_layout()
plt.show()

# Printing accuracies
print("Accuracy of Logistic Regression: {:.2f}%".format(accuracy_lr * 100))
print("Accuracy of Decision Tree: {:.2f}%".format(accuracy_dt * 100))
print("Accuracy of SVM: {:.2f}%".format(accuracy_svm * 100))
print("Accuracy of KNN: {:.2f}%".format(accuracy_knn * 100))


```

### Conclusion

In this post, we've explored various classification algorithms, each with its unique strengths. Choosing the right classifier depends on the dataset's size, quality, and nature. Experimentation and cross-validation are key to determining the most effective model for your specific data.