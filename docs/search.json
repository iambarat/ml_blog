[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Mohaimin Al Barat, a Ph.D. student in Computer Science at Virginia Tech."
  },
  {
    "objectID": "posts/regression/index.html",
    "href": "posts/regression/index.html",
    "title": "Exploring Linear and Non-Linear Regression",
    "section": "",
    "text": "Regression analysis is a powerful statistical method used for modeling the relationship between a dependent variable and one or more independent variables. In this blog post, we will explore two primary types of regression: linear and non-linear, using Python within the RStudio environment.\n\n\nLinear regression is the most straightforward approach in regression analysis. It assumes a linear relationship between the input variables (independent) and the single output variable (dependent).\n\n\nThe linear regression model can be represented by:\n\\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\epsilon \\]\nwhere \\(\\beta_0, \\beta_1, ..., \\beta_n\\) are coefficients, and \\(\\epsilon\\) is the error term.\n\n\n\nLet’s start with a simple example of linear regression with one independent variable.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Sample data\nX = np.array([[5], [15], [25], [35], [45], [55]])\ny = np.array([5, 20, 14, 32, 22, 38])\n\n# Creating the linear regression model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Predictions\nX_new = np.array([[65]])\ny_pred = model.predict(X_new)\n\n# Plotting\nplt.scatter(X, y, color='blue')\nplt.plot(X, model.predict(X), color='red')\nplt.scatter(X_new, y_pred, color='green')\nplt.title('Simple Linear Regression')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n\n\n\n\n\n\n\n\nIn contrast, non-linear regression is used when the data shows a more complex relationship that cannot be accurately described by a linear model. Non-linear regression can take many forms, one of which is the polynomial regression, represented by: \\[ y = \\beta_0 + \\beta_1 x^1 + \\beta_2 x^2 + ... + \\beta_n x^n + \\epsilon \\]\nBelow is an example of non linear regression using polynomial model.\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Transforming the data\npolynomial_features = PolynomialFeatures(degree=2)\nX_poly = polynomial_features.fit_transform(X)\n\n# Creating a linear regression model\nmodel = LinearRegression()\nmodel.fit(X_poly, y)\n\n# Predictions\nX_new_poly = polynomial_features.fit_transform(X_new)\ny_poly_pred = model.predict(X_new_poly)\n\n# Plotting\nplt.scatter(X, y, color='blue')\nplt.plot(X, model.predict(X_poly), color='red')\nplt.scatter(X_new, y_poly_pred, color='green')\nplt.title('Polynomial Regression')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n\n\n\n\n\n\n\nUnderstanding the difference between linear and non-linear regression is crucial for accurately modeling data in various fields. Python provides a versatile and approachable platform for implementing and visualizing both types of regression, making it an excellent choice for data scientists and statisticians."
  },
  {
    "objectID": "posts/regression/index.html#linear-regression",
    "href": "posts/regression/index.html#linear-regression",
    "title": "Exploring Linear and Non-Linear Regression",
    "section": "",
    "text": "Linear regression is the most straightforward approach in regression analysis. It assumes a linear relationship between the input variables (independent) and the single output variable (dependent).\n\n\nThe linear regression model can be represented by:\n\\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\epsilon \\]\nwhere \\(\\beta_0, \\beta_1, ..., \\beta_n\\) are coefficients, and \\(\\epsilon\\) is the error term.\n\n\n\nLet’s start with a simple example of linear regression with one independent variable.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Sample data\nX = np.array([[5], [15], [25], [35], [45], [55]])\ny = np.array([5, 20, 14, 32, 22, 38])\n\n# Creating the linear regression model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Predictions\nX_new = np.array([[65]])\ny_pred = model.predict(X_new)\n\n# Plotting\nplt.scatter(X, y, color='blue')\nplt.plot(X, model.predict(X), color='red')\nplt.scatter(X_new, y_pred, color='green')\nplt.title('Simple Linear Regression')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()"
  },
  {
    "objectID": "posts/regression/index.html#non-linear-regression",
    "href": "posts/regression/index.html#non-linear-regression",
    "title": "Exploring Linear and Non-Linear Regression",
    "section": "",
    "text": "In contrast, non-linear regression is used when the data shows a more complex relationship that cannot be accurately described by a linear model. Non-linear regression can take many forms, one of which is the polynomial regression, represented by: \\[ y = \\beta_0 + \\beta_1 x^1 + \\beta_2 x^2 + ... + \\beta_n x^n + \\epsilon \\]\nBelow is an example of non linear regression using polynomial model.\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Transforming the data\npolynomial_features = PolynomialFeatures(degree=2)\nX_poly = polynomial_features.fit_transform(X)\n\n# Creating a linear regression model\nmodel = LinearRegression()\nmodel.fit(X_poly, y)\n\n# Predictions\nX_new_poly = polynomial_features.fit_transform(X_new)\ny_poly_pred = model.predict(X_new_poly)\n\n# Plotting\nplt.scatter(X, y, color='blue')\nplt.plot(X, model.predict(X_poly), color='red')\nplt.scatter(X_new, y_poly_pred, color='green')\nplt.title('Polynomial Regression')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()"
  },
  {
    "objectID": "posts/regression/index.html#conclusion",
    "href": "posts/regression/index.html#conclusion",
    "title": "Exploring Linear and Non-Linear Regression",
    "section": "",
    "text": "Understanding the difference between linear and non-linear regression is crucial for accurately modeling data in various fields. Python provides a versatile and approachable platform for implementing and visualizing both types of regression, making it an excellent choice for data scientists and statisticians."
  },
  {
    "objectID": "posts/ids/index.html",
    "href": "posts/ids/index.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Anomaly detection, also known as outlier detection, is a fascinating area of machine learning aimed at identifying abnormal patterns that do not conform to expected behavior. These anomalies can be crucial in various domains like fraud detection, system health monitoring, and outlier detection in data cleaning and preprocessing.\nIn this blog, we’ll dive deep into the world of anomaly detection, exploring its concepts, methodologies, and implementation using Python.\n\n\nAnomaly detection focuses on identifying data points, events, or observations that deviate significantly from the dataset’s norm. Mathematically, if we consider a dataset \\(D\\) containing elements \\(x_1, x_2, ..., x_n\\), an anomaly \\(x_i\\) is a point that deviates significantly from the majority of points in \\(D\\).\n\n\n\nPoint Anomalies: Single data points that are far from the rest of the data.\nContextual Anomalies: Abnormalities in a specific context but not otherwise.\nCollective Anomalies: A collection of data points that are anomalous in relation to the entire dataset.\n\n\n\n\n\nWe’ll use the scikit-learn library in Python to demonstrate anomaly detection. Ensure you have it installed:\npip install scikit-learn\nWe’ll use a simple dataset and Isolation Forest algorithm to detect anomaly for this example.\n\nimport numpy as np\nfrom sklearn.ensemble import IsolationForest\nimport matplotlib.pyplot as plt\n\n# Generating sample data\nrng = np.random.RandomState(42)\nX = 0.3 * rng.randn(100, 2)\nX_train = np.r_[X + 2, X - 2]\n\n# Adding outliers\nX_outliers = rng.uniform(low=-4, high=4, size=(20, 2))\nX_train = np.r_[X_train, X_outliers]\n\n# Fitting the model\nclf = IsolationForest(max_samples=100, random_state=rng)\nclf.fit(X_train)\n\n# Predictions\ny_pred_train = clf.predict(X_train)\n\n# Visualizing the data\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_pred_train, cmap='Paired')\nplt.title(\"Isolation Forest Anomaly Detection\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.show()\n\n\n\n\nIn this example, the Isolation Forest algorithm efficiently separates the outliers from the normal observations.\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import NearestNeighbors\n\n# Creating sample data with some anomalies\nnp.random.seed(0)\nX_normal = np.random.rand(100, 2) * 2\nX_anomaly = np.array([[3, 3], [4, 4], [5, 5], [2.5, 3], [3, 2.5]])  # Adding more anomalies\nX = np.concatenate([X_normal, X_anomaly], axis=0)\n\n# Fitting the KNN model\nknn = NearestNeighbors(n_neighbors=2)\nknn.fit(X)\n\n# Finding the distances and indices of the nearest neighbors\ndistances, indices = knn.kneighbors(X)\n\n# Threshold for anomaly (set as a distance that seems to separate normal points from anomalies)\nthreshold = 1.0\n\n# Detecting anomalies\nis_anomaly = np.any(distances &gt; threshold, axis=1)\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.scatter(X[is_anomaly, 0], X[is_anomaly, 1], color='red', label='Anomaly')\nplt.scatter(X[~is_anomaly, 0], X[~is_anomaly, 1], color='blue', label='Normal')\nplt.title('KNN Anomaly Detection')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n\n\n\n\nIn this example, the KNN algorithm efficiently separates the outliers from the normal observations.\n\n\n\n\nAnomaly detection is a powerful tool in machine learning, offering significant benefits in various applications. Python’s ‘scikit-learn’ provides accessible and efficient tools to implement these techniques. As with any machine learning task, the key to success lies in understanding your data and choosing the right algorithm for your specific use case."
  },
  {
    "objectID": "posts/ids/index.html#fundamentals-of-anomaly-detection",
    "href": "posts/ids/index.html#fundamentals-of-anomaly-detection",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Anomaly detection focuses on identifying data points, events, or observations that deviate significantly from the dataset’s norm. Mathematically, if we consider a dataset \\(D\\) containing elements \\(x_1, x_2, ..., x_n\\), an anomaly \\(x_i\\) is a point that deviates significantly from the majority of points in \\(D\\).\n\n\n\nPoint Anomalies: Single data points that are far from the rest of the data.\nContextual Anomalies: Abnormalities in a specific context but not otherwise.\nCollective Anomalies: A collection of data points that are anomalous in relation to the entire dataset."
  },
  {
    "objectID": "posts/ids/index.html#implementing-anomaly-detection-in-python",
    "href": "posts/ids/index.html#implementing-anomaly-detection-in-python",
    "title": "Anomaly Detection",
    "section": "",
    "text": "We’ll use the scikit-learn library in Python to demonstrate anomaly detection. Ensure you have it installed:\npip install scikit-learn\nWe’ll use a simple dataset and Isolation Forest algorithm to detect anomaly for this example.\n\nimport numpy as np\nfrom sklearn.ensemble import IsolationForest\nimport matplotlib.pyplot as plt\n\n# Generating sample data\nrng = np.random.RandomState(42)\nX = 0.3 * rng.randn(100, 2)\nX_train = np.r_[X + 2, X - 2]\n\n# Adding outliers\nX_outliers = rng.uniform(low=-4, high=4, size=(20, 2))\nX_train = np.r_[X_train, X_outliers]\n\n# Fitting the model\nclf = IsolationForest(max_samples=100, random_state=rng)\nclf.fit(X_train)\n\n# Predictions\ny_pred_train = clf.predict(X_train)\n\n# Visualizing the data\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_pred_train, cmap='Paired')\nplt.title(\"Isolation Forest Anomaly Detection\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.show()\n\n\n\n\nIn this example, the Isolation Forest algorithm efficiently separates the outliers from the normal observations.\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import NearestNeighbors\n\n# Creating sample data with some anomalies\nnp.random.seed(0)\nX_normal = np.random.rand(100, 2) * 2\nX_anomaly = np.array([[3, 3], [4, 4], [5, 5], [2.5, 3], [3, 2.5]])  # Adding more anomalies\nX = np.concatenate([X_normal, X_anomaly], axis=0)\n\n# Fitting the KNN model\nknn = NearestNeighbors(n_neighbors=2)\nknn.fit(X)\n\n# Finding the distances and indices of the nearest neighbors\ndistances, indices = knn.kneighbors(X)\n\n# Threshold for anomaly (set as a distance that seems to separate normal points from anomalies)\nthreshold = 1.0\n\n# Detecting anomalies\nis_anomaly = np.any(distances &gt; threshold, axis=1)\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.scatter(X[is_anomaly, 0], X[is_anomaly, 1], color='red', label='Anomaly')\nplt.scatter(X[~is_anomaly, 0], X[~is_anomaly, 1], color='blue', label='Normal')\nplt.title('KNN Anomaly Detection')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n\n\n\n\nIn this example, the KNN algorithm efficiently separates the outliers from the normal observations."
  },
  {
    "objectID": "posts/ids/index.html#conclusion",
    "href": "posts/ids/index.html#conclusion",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Anomaly detection is a powerful tool in machine learning, offering significant benefits in various applications. Python’s ‘scikit-learn’ provides accessible and efficient tools to implement these techniques. As with any machine learning task, the key to success lies in understanding your data and choosing the right algorithm for your specific use case."
  },
  {
    "objectID": "posts/classification/index.html",
    "href": "posts/classification/index.html",
    "title": "Understanding Classification",
    "section": "",
    "text": "Classification is a cornerstone of machine learning, where we teach a computer how to make decisions from data. It involves categorizing data into predefined classes. In this post, we’ll explore the basics of classification in machine learning using Python, a popular language in data science due to its simplicity and robust libraries.\n\n\nClassification algorithms represent a fundamental aspect of supervised learning in machine learning. These algorithms focus on categorizing new observations into predefined classes or groups based on a dataset of labeled examples. In simpler terms, classification involves using these algorithms to predict the category or class of new, unseen instances.\nThe essence of classification is to “learn” from a dataset where the categories (or labels) of observations are known, and then apply this learned pattern to predict the class of new observations. Common examples include binary classifications like Yes or No, 0 or 1, Spam or Not Spam, and multi-class scenarios like distinguishing between different types of animals, colors, or objects. In contrast to regression, where the output is a continuous value, the output of a classification algorithm is categorical. These categories, also known as labels or targets, are discrete and typically non-numeric. For instance, categories might be “Red” or “Blue”, “Cat” or “Dog”, “Spam” or “Not Spam”.\nAs a supervised learning technique, classification relies on labeled input data. This means that the training data includes both the input features and the corresponding output labels. The algorithm learns to associate specific input patterns with particular output categories, enabling it to make predictions for new, unlabeled data.\n\n\nBinary Classification: Involves two classes. For example, determining whether an email is spam.\nMulticlass Classification: Involves more than two classes. For instance, classifying types of crops.\n\n\n\nWe’ll use the Iris dataset, a classic in machine learning, which includes data on various iris flowers and classifies them into three species.\n\n#loading dataset\nfrom sklearn.datasets import load_iris\niris = load_iris()\n\n#splitting dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data, iris.target, test_size=0.3, random_state=42)\n\n#knn\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\n\n#knn_evaluation\nfrom sklearn.metrics import accuracy_score\ny_pred = knn.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n\n### Decision Tree Classifier\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\n\n#evaluation\ny_pred_dt = decision_tree.predict(X_test)\naccuracy_dt = accuracy_score(y_test, y_pred_dt)\nprint(f\"Decision Tree Accuracy: {accuracy_dt:.2f}\")\n\n#support vector machine classifier\nfrom sklearn.svm import SVC\n\n#training\nsvm = SVC()\nsvm.fit(X_train, y_train)\n\n#evaluating\ny_pred_svm = svm.predict(X_test)\naccuracy_svm = accuracy_score(y_test, y_pred_svm)\nprint(f\"SVM Accuracy: {accuracy_svm:.2f}\")\n\n\n# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(max_iter=200)\nlog_reg.fit(X_train, y_train)\n\n# Evaluating\ny_pred_lr = log_reg.predict(X_test)\naccuracy_lr = accuracy_score(y_test, y_pred_lr)\nprint(f\"Logistic Regression Accuracy: {accuracy_lr:.2f}\")\n\nAccuracy: 1.00\nDecision Tree Accuracy: 1.00\nSVM Accuracy: 1.00\nLogistic Regression Accuracy: 1.00\n\n\n\n\n\nIn this post, we’ve explored various classification algorithms, each with its unique strengths. Choosing the right classifier depends on the dataset’s size, quality, and nature. Experimentation and cross-validation are key to determining the most effective model for your specific data."
  },
  {
    "objectID": "posts/classification/index.html#classification-basics",
    "href": "posts/classification/index.html#classification-basics",
    "title": "Understanding Classification",
    "section": "",
    "text": "Classification algorithms represent a fundamental aspect of supervised learning in machine learning. These algorithms focus on categorizing new observations into predefined classes or groups based on a dataset of labeled examples. In simpler terms, classification involves using these algorithms to predict the category or class of new, unseen instances.\nThe essence of classification is to “learn” from a dataset where the categories (or labels) of observations are known, and then apply this learned pattern to predict the class of new observations. Common examples include binary classifications like Yes or No, 0 or 1, Spam or Not Spam, and multi-class scenarios like distinguishing between different types of animals, colors, or objects. In contrast to regression, where the output is a continuous value, the output of a classification algorithm is categorical. These categories, also known as labels or targets, are discrete and typically non-numeric. For instance, categories might be “Red” or “Blue”, “Cat” or “Dog”, “Spam” or “Not Spam”.\nAs a supervised learning technique, classification relies on labeled input data. This means that the training data includes both the input features and the corresponding output labels. The algorithm learns to associate specific input patterns with particular output categories, enabling it to make predictions for new, unlabeled data.\n\n\nBinary Classification: Involves two classes. For example, determining whether an email is spam.\nMulticlass Classification: Involves more than two classes. For instance, classifying types of crops.\n\n\n\nWe’ll use the Iris dataset, a classic in machine learning, which includes data on various iris flowers and classifies them into three species.\n\n#loading dataset\nfrom sklearn.datasets import load_iris\niris = load_iris()\n\n#splitting dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data, iris.target, test_size=0.3, random_state=42)\n\n#knn\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\n\n#knn_evaluation\nfrom sklearn.metrics import accuracy_score\ny_pred = knn.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n\n### Decision Tree Classifier\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\n\n#evaluation\ny_pred_dt = decision_tree.predict(X_test)\naccuracy_dt = accuracy_score(y_test, y_pred_dt)\nprint(f\"Decision Tree Accuracy: {accuracy_dt:.2f}\")\n\n#support vector machine classifier\nfrom sklearn.svm import SVC\n\n#training\nsvm = SVC()\nsvm.fit(X_train, y_train)\n\n#evaluating\ny_pred_svm = svm.predict(X_test)\naccuracy_svm = accuracy_score(y_test, y_pred_svm)\nprint(f\"SVM Accuracy: {accuracy_svm:.2f}\")\n\n\n# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(max_iter=200)\nlog_reg.fit(X_train, y_train)\n\n# Evaluating\ny_pred_lr = log_reg.predict(X_test)\naccuracy_lr = accuracy_score(y_test, y_pred_lr)\nprint(f\"Logistic Regression Accuracy: {accuracy_lr:.2f}\")\n\nAccuracy: 1.00\nDecision Tree Accuracy: 1.00\nSVM Accuracy: 1.00\nLogistic Regression Accuracy: 1.00\n\n\n\n\n\nIn this post, we’ve explored various classification algorithms, each with its unique strengths. Choosing the right classifier depends on the dataset’s size, quality, and nature. Experimentation and cross-validation are key to determining the most effective model for your specific data."
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Understanding Clustering in Machine Learning",
    "section": "",
    "text": "Clustering is a fundamental technique in machine learning, often used in data analysis for pattern recognition and grouping similar data points. In this blog post, we’ll explore the basics of clustering, delve into some common algorithms, and provide Python examples to illustrate these concepts in practice.\n\n\nClustering is an unsupervised learning technique used to group a set of objects in such a way that objects in the same group (a cluster) are more similar to each other than to those in other groups.\n\n\n\nCluster: A collection of data points aggregated together because of certain similarities.\nCentroid: The central point of a cluster. Not all clustering algorithms use centroids.\n\n\n\n\n\n\nK-Means Clustering: Groups data into (k) number of clusters.\nHierarchical Clustering: Builds a hierarchy of clusters using a tree-like structure.\n\n\n\n\n\n\nThe goal of K-means is to minimize the within-cluster variance, defined as:\n\\[ W(C_k) = \\sum_{x_i \\in C_k} ||x_i - \\mu_k||^2 \\]\nwhere \\(C_k\\) is the (k)th cluster, \\(x_i\\) is a data point, and \\(\\mu_k\\) is the centroid of \\(C_k\\).\n\n\n\n\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Load the iris dataset\niris = datasets.load_iris()\nX_iris = iris.data[:, :2]  # We only take the first two features for simplicity\n\n# KMeans clustering\nkmeans_iris = KMeans(n_clusters=2, random_state=0).fit(X_iris)\n\n# Plotting the clusters\nplt.scatter(X_iris[:, 0], X_iris[:, 1], c=kmeans_iris.labels_, cmap='rainbow')\nplt.title('K-Means Clustering on Iris Dataset')\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.show()\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\nThe resulting plot shows how K-Means algorithm clusters the iris flowers based on sepal length and width. You can experiment with different numbers of clusters ‘n_clusters’ and features to see how the clustering changes.\n\n\n\n\nClustering offers a way to uncover hidden patterns in data, making it a crucial tool in the machine learning toolkit. Python, with its rich ecosystem of libraries, provides an accessible platform for exploring these techniques."
  },
  {
    "objectID": "posts/clustering/index.html#what-is-clustering",
    "href": "posts/clustering/index.html#what-is-clustering",
    "title": "Understanding Clustering in Machine Learning",
    "section": "",
    "text": "Clustering is an unsupervised learning technique used to group a set of objects in such a way that objects in the same group (a cluster) are more similar to each other than to those in other groups.\n\n\n\nCluster: A collection of data points aggregated together because of certain similarities.\nCentroid: The central point of a cluster. Not all clustering algorithms use centroids."
  },
  {
    "objectID": "posts/clustering/index.html#common-clustering-algorithms",
    "href": "posts/clustering/index.html#common-clustering-algorithms",
    "title": "Understanding Clustering in Machine Learning",
    "section": "",
    "text": "K-Means Clustering: Groups data into (k) number of clusters.\nHierarchical Clustering: Builds a hierarchy of clusters using a tree-like structure."
  },
  {
    "objectID": "posts/clustering/index.html#k-means-clustering",
    "href": "posts/clustering/index.html#k-means-clustering",
    "title": "Understanding Clustering in Machine Learning",
    "section": "",
    "text": "The goal of K-means is to minimize the within-cluster variance, defined as:\n\\[ W(C_k) = \\sum_{x_i \\in C_k} ||x_i - \\mu_k||^2 \\]\nwhere \\(C_k\\) is the (k)th cluster, \\(x_i\\) is a data point, and \\(\\mu_k\\) is the centroid of \\(C_k\\).\n\n\n\n\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Load the iris dataset\niris = datasets.load_iris()\nX_iris = iris.data[:, :2]  # We only take the first two features for simplicity\n\n# KMeans clustering\nkmeans_iris = KMeans(n_clusters=2, random_state=0).fit(X_iris)\n\n# Plotting the clusters\nplt.scatter(X_iris[:, 0], X_iris[:, 1], c=kmeans_iris.labels_, cmap='rainbow')\nplt.title('K-Means Clustering on Iris Dataset')\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.show()\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\nThe resulting plot shows how K-Means algorithm clusters the iris flowers based on sepal length and width. You can experiment with different numbers of clusters ‘n_clusters’ and features to see how the clustering changes."
  },
  {
    "objectID": "posts/clustering/index.html#conclusion",
    "href": "posts/clustering/index.html#conclusion",
    "title": "Understanding Clustering in Machine Learning",
    "section": "",
    "text": "Clustering offers a way to uncover hidden patterns in data, making it a crucial tool in the machine learning toolkit. Python, with its rich ecosystem of libraries, provides an accessible platform for exploring these techniques."
  },
  {
    "objectID": "posts/random_variable/index.html",
    "href": "posts/random_variable/index.html",
    "title": "Random Variables",
    "section": "",
    "text": "Random variables and probability distribution are fundamental concepts in statistics, forming the backbone of data analysis and probabilistic modeling. In this blog, we explore these concepts including the basics, some examples, and visualization of the outputs.\n\n\n\nA random variable is a variable whose possible values are numerical outcomes of a random phenomenon. In machine learning, they are key to modeling uncertainties and probabilities.\n\n\n\nPython, with libraries like numpy and matplotlib, is a powerful tool for statistical simulations. Let’s demonstrate this with some examples.\n\n\nFirst, we’ll simulate a discrete random variable: the outcome of a dice roll. \n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Setting a seed for reproducibility\nnp.random.seed(123)\n\n# Simulating dice rolls\ndice_rolls = np.random.choice([1, 2, 3, 4, 5, 6], size=1000)\n\n# Plotting the results\nplt.hist(dice_rolls, bins=np.arange(1, 8) - 0.5, rwidth=0.8)\nplt.title('Dice Roll Distribution')\nplt.xlabel('Dice Value')\nplt.ylabel('Frequency')\nplt.xticks(range(1, 7))\n\n([&lt;matplotlib.axis.XTick object at 0x130a9ebd0&gt;, &lt;matplotlib.axis.XTick object at 0x130871f10&gt;, &lt;matplotlib.axis.XTick object at 0x130aae250&gt;, &lt;matplotlib.axis.XTick object at 0x130a65850&gt;, &lt;matplotlib.axis.XTick object at 0x130af8d50&gt;, &lt;matplotlib.axis.XTick object at 0x130afaf50&gt;], [Text(1, 0, '1'), Text(2, 0, '2'), Text(3, 0, '3'), Text(4, 0, '4'), Text(5, 0, '5'), Text(6, 0, '6')])\n\nplt.show()\n\n\n\n# Simulating a normal distribution for continuous random variable\nnormal_data = np.random.normal(loc=0, scale=1, size=1000)\n\n# Plotting the results\nplt.hist(normal_data, bins=30, density=True, alpha=0.6, color='g')\nplt.title('Normal Distribution')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.show()\n\n\n\n\n\n\n\n\nA probability function assigns probabilities to each possible outcome of a random experiment. The function must satisfy three conditions: non-negativity, normalization, and countable additivity.\nMathematically, a probability function ( P ) for a random variable ( X ) is defined as:\n\\[ P(X = x) \\geq 0 \\]\n\\[ \\sum P(X = x_i) = 1 \\]\nWhere \\(x_i\\) are the possible outcomes of ( X ).\nProbability distributions describe the likelihood of each possible value of a random variable. There are two main types of probability distributions: discrete and continuous.\n\n\nThe PMF is a function that describes the probabilities of different outcomes of a discrete random variable. The PMF assigns a value to each possible outcome, which represents the probability that the outcome will occur. The PMF is defined as the probability of a random variable taking on a particular value. The sum of all the PMF values is equal to 1, as the probability of the outcome must be 100%.\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# PMF of a fair die\nvalues = np.arange(1, 7)\nprobabilities = np.full(6, 1/6)\n\nplt.bar(values, probabilities)\nplt.title('PMF of a Fair Die')\nplt.xlabel('Die Value')\nplt.ylabel('Probability')\nplt.show()\n\n\n\n\n\n\n\n\nThe Cumulative Distribution Function (CDF) is a fundamental concept in probability theory that applies to both discrete and continuous random variables. It represents the probability that a random variable (X) will take on a value less than or equal to a specific value (x). \\[F_X(x)=P(X\\leq x)\\] ### Example: CDF of a Normal Distribution\n\n# CDF of a standard normal distribution\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nx = np.linspace(-3, 3, 1000)\ny = norm.pdf(x, 0, 1)\n\ny_cdf = norm.cdf(x, 0, 1)\n\nplt.plot(x, y_cdf)\nplt.title('CDF of Standard Normal Distribution')\nplt.xlabel('Value')\nplt.ylabel('Cumulative Probability')\nplt.show()\n\n\n\n\n\n\n\nUnlike the PMF, which is used for discrete random variables, the Probability Density Function (PDF) is associated with continuous random variables. The PDF describes the relative likelihood of a continuous random variable to take on a specific value.\n\n\nThe key distinction to understand is that, for continuous random variables, the probability of taking on any single, exact value is effectively zero, as there are infinitely many possible values. Therefore, the PDF does not directly give us a probability for a single value. Instead, it helps us understand the distribution of probabilities across a range of values.\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# PDF of a standard normal distribution\nx = np.linspace(-3, 3, 1000)\ny = norm.pdf(x, 0, 1)\n\nplt.plot(x, y)\nplt.title('PDF of Standard Normal Distribution')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.show()\n\n\n\n\n\n\n\nUnderstanding the concepts of random variables and probability function are essential in the field of machine learning and data science. The PMF, PDF, CDF functions provide fundamental insights into the behavior of random variables."
  },
  {
    "objectID": "posts/random_variable/index.html#introduction",
    "href": "posts/random_variable/index.html#introduction",
    "title": "Random Variables",
    "section": "",
    "text": "Random variables and probability distribution are fundamental concepts in statistics, forming the backbone of data analysis and probabilistic modeling. In this blog, we explore these concepts including the basics, some examples, and visualization of the outputs."
  },
  {
    "objectID": "posts/random_variable/index.html#random-variable",
    "href": "posts/random_variable/index.html#random-variable",
    "title": "Random Variables",
    "section": "",
    "text": "A random variable is a variable whose possible values are numerical outcomes of a random phenomenon. In machine learning, they are key to modeling uncertainties and probabilities."
  },
  {
    "objectID": "posts/random_variable/index.html#simulating-random-variables-in-python",
    "href": "posts/random_variable/index.html#simulating-random-variables-in-python",
    "title": "Random Variables",
    "section": "",
    "text": "Python, with libraries like numpy and matplotlib, is a powerful tool for statistical simulations. Let’s demonstrate this with some examples.\n\n\nFirst, we’ll simulate a discrete random variable: the outcome of a dice roll. \n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Setting a seed for reproducibility\nnp.random.seed(123)\n\n# Simulating dice rolls\ndice_rolls = np.random.choice([1, 2, 3, 4, 5, 6], size=1000)\n\n# Plotting the results\nplt.hist(dice_rolls, bins=np.arange(1, 8) - 0.5, rwidth=0.8)\nplt.title('Dice Roll Distribution')\nplt.xlabel('Dice Value')\nplt.ylabel('Frequency')\nplt.xticks(range(1, 7))\n\n([&lt;matplotlib.axis.XTick object at 0x130a9ebd0&gt;, &lt;matplotlib.axis.XTick object at 0x130871f10&gt;, &lt;matplotlib.axis.XTick object at 0x130aae250&gt;, &lt;matplotlib.axis.XTick object at 0x130a65850&gt;, &lt;matplotlib.axis.XTick object at 0x130af8d50&gt;, &lt;matplotlib.axis.XTick object at 0x130afaf50&gt;], [Text(1, 0, '1'), Text(2, 0, '2'), Text(3, 0, '3'), Text(4, 0, '4'), Text(5, 0, '5'), Text(6, 0, '6')])\n\nplt.show()\n\n\n\n# Simulating a normal distribution for continuous random variable\nnormal_data = np.random.normal(loc=0, scale=1, size=1000)\n\n# Plotting the results\nplt.hist(normal_data, bins=30, density=True, alpha=0.6, color='g')\nplt.title('Normal Distribution')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.show()"
  },
  {
    "objectID": "posts/random_variable/index.html#probability-distribution",
    "href": "posts/random_variable/index.html#probability-distribution",
    "title": "Random Variables",
    "section": "",
    "text": "A probability function assigns probabilities to each possible outcome of a random experiment. The function must satisfy three conditions: non-negativity, normalization, and countable additivity.\nMathematically, a probability function ( P ) for a random variable ( X ) is defined as:\n\\[ P(X = x) \\geq 0 \\]\n\\[ \\sum P(X = x_i) = 1 \\]\nWhere \\(x_i\\) are the possible outcomes of ( X ).\nProbability distributions describe the likelihood of each possible value of a random variable. There are two main types of probability distributions: discrete and continuous.\n\n\nThe PMF is a function that describes the probabilities of different outcomes of a discrete random variable. The PMF assigns a value to each possible outcome, which represents the probability that the outcome will occur. The PMF is defined as the probability of a random variable taking on a particular value. The sum of all the PMF values is equal to 1, as the probability of the outcome must be 100%.\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# PMF of a fair die\nvalues = np.arange(1, 7)\nprobabilities = np.full(6, 1/6)\n\nplt.bar(values, probabilities)\nplt.title('PMF of a Fair Die')\nplt.xlabel('Die Value')\nplt.ylabel('Probability')\nplt.show()"
  },
  {
    "objectID": "posts/random_variable/index.html#cumulative-density-function-cdf",
    "href": "posts/random_variable/index.html#cumulative-density-function-cdf",
    "title": "Random Variables",
    "section": "",
    "text": "The Cumulative Distribution Function (CDF) is a fundamental concept in probability theory that applies to both discrete and continuous random variables. It represents the probability that a random variable (X) will take on a value less than or equal to a specific value (x). \\[F_X(x)=P(X\\leq x)\\] ### Example: CDF of a Normal Distribution\n\n# CDF of a standard normal distribution\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nx = np.linspace(-3, 3, 1000)\ny = norm.pdf(x, 0, 1)\n\ny_cdf = norm.cdf(x, 0, 1)\n\nplt.plot(x, y_cdf)\nplt.title('CDF of Standard Normal Distribution')\nplt.xlabel('Value')\nplt.ylabel('Cumulative Probability')\nplt.show()"
  },
  {
    "objectID": "posts/random_variable/index.html#probability-density-function-pdf",
    "href": "posts/random_variable/index.html#probability-density-function-pdf",
    "title": "Random Variables",
    "section": "",
    "text": "Unlike the PMF, which is used for discrete random variables, the Probability Density Function (PDF) is associated with continuous random variables. The PDF describes the relative likelihood of a continuous random variable to take on a specific value.\n\n\nThe key distinction to understand is that, for continuous random variables, the probability of taking on any single, exact value is effectively zero, as there are infinitely many possible values. Therefore, the PDF does not directly give us a probability for a single value. Instead, it helps us understand the distribution of probabilities across a range of values.\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# PDF of a standard normal distribution\nx = np.linspace(-3, 3, 1000)\ny = norm.pdf(x, 0, 1)\n\nplt.plot(x, y)\nplt.title('PDF of Standard Normal Distribution')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.show()\n\n\n\n\n\n\n\nUnderstanding the concepts of random variables and probability function are essential in the field of machine learning and data science. The PMF, PDF, CDF functions provide fundamental insights into the behavior of random variables."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blog",
    "section": "",
    "text": "Random Variables and Probability Distribution\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nMohaimin\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding Clustering in Machine Learning\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nYour Name\n\n\n\n\n\n\n  \n\n\n\n\nExploring Linear and Non-Linear Regression\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nMohaimin\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding Classification\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nMohaimin\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly Detection\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nMohaimin\n\n\n\n\n\n\nNo matching items"
  }
]