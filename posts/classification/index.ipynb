{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Understanding Classification\"\n",
        "author: \"Mohaimin\"\n",
        "date: \"2023-12-03\"\n",
        "format: html\n",
        "categories: [classification]\n",
        "---"
      ],
      "id": "f6a5ef0b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Classification in Machine Learning\n",
        "\n",
        "Classification is a cornerstone of machine learning, where we teach a computer how to make decisions from data. It involves categorizing data into predefined classes. In this post, we'll explore the basics of classification in machine learning using Python, a popular language in data science due to its simplicity and robust libraries.\n",
        "\n",
        "\n",
        "## Classification Basics\n",
        "\n",
        "Classification algorithms represent a fundamental aspect of supervised learning in machine learning. These algorithms focus on categorizing new observations into predefined classes or groups based on a dataset of labeled examples. In simpler terms, classification involves using these algorithms to predict the category or class of new, unseen instances.\n",
        "\n",
        "The essence of classification is to \"learn\" from a dataset where the categories (or labels) of observations are known, and then apply this learned pattern to predict the class of new observations. Common examples include binary classifications like Yes or No, 0 or 1, Spam or Not Spam, and multi-class scenarios like distinguishing between different types of animals, colors, or objects. In contrast to regression, where the output is a continuous value, the output of a classification algorithm is categorical. These categories, also known as labels or targets, are discrete and typically non-numeric. For instance, categories might be \"Red\" or \"Blue\", \"Cat\" or \"Dog\", \"Spam\" or \"Not Spam\". \n",
        "\n",
        "As a supervised learning technique, classification relies on labeled input data. This means that the training data includes both the input features and the corresponding output labels. The algorithm learns to associate specific input patterns with particular output categories, enabling it to make predictions for new, unlabeled data.\n",
        "\n",
        "\n",
        "### Types of Classification\n",
        "\n",
        "**Binary Classification:** Involves two classes. For example, determining whether an email is spam. \n",
        "\n",
        "**Multiclass Classification:** Involves more than two classes. For instance, classifying types of crops.\n",
        "\n",
        "### Implementing Classification in Python\n",
        "\n",
        "We'll use the Iris dataset, a classic in machine learning, which includes data on various iris flowers and classifies them into three species.\n"
      ],
      "id": "107c996b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# For simplicity, using only two classes\n",
        "X, y = X[y != 2], y[y != 2]\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Logistic Regression Classifier\n",
        "clf_lr = LogisticRegression()\n",
        "clf_lr.fit(X_train, y_train)\n",
        "y_pred_lr = clf_lr.predict(X_test)\n",
        "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "\n",
        "# Decision Tree Classifier\n",
        "clf_dt = DecisionTreeClassifier()\n",
        "clf_dt.fit(X_train, y_train)\n",
        "y_pred_dt = clf_dt.predict(X_test)\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# SVM Classifier\n",
        "clf_svm = SVC(probability=True)\n",
        "clf_svm.fit(X_train, y_train)\n",
        "y_pred_svm = clf_svm.predict(X_test)\n",
        "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "\n",
        "# KNN Classifier\n",
        "clf_knn = KNeighborsClassifier(n_neighbors=3)\n",
        "clf_knn.fit(X_train, y_train)\n",
        "y_pred_knn = clf_knn.predict(X_test)\n",
        "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
        "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
        "\n",
        "# Plotting Confusion Matrices for all classifiers\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
        "\n",
        "# Logistic Regression\n",
        "sns.heatmap(cm_lr, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[0, 0])\n",
        "axes[0, 0].set_title('Logistic Regression Confusion Matrix')\n",
        "axes[0, 0].set_xlabel('Predicted Label')\n",
        "axes[0, 0].set_ylabel('True Label')\n",
        "\n",
        "# Decision Tree\n",
        "sns.heatmap(cm_dt, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Decision Tree Confusion Matrix')\n",
        "axes[0, 1].set_xlabel('Predicted Label')\n",
        "axes[0, 1].set_ylabel('True Label')\n",
        "\n",
        "# SVM\n",
        "sns.heatmap(cm_svm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[1, 0])\n",
        "axes[1, 0].set_title('SVM Confusion Matrix')\n",
        "axes[1, 0].set_xlabel('Predicted Label')\n",
        "axes[1, 0].set_ylabel('True Label')\n",
        "\n",
        "# KNN\n",
        "sns.heatmap(cm_knn, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[1, 1])\n",
        "axes[1, 1].set_title('KNN Confusion Matrix')\n",
        "axes[1, 1].set_xlabel('Predicted Label')\n",
        "axes[1, 1].set_ylabel('True Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Printing accuracies\n",
        "print(\"Accuracy of Logistic Regression: {:.2f}%\".format(accuracy_lr * 100))\n",
        "print(\"Accuracy of Decision Tree: {:.2f}%\".format(accuracy_dt * 100))\n",
        "print(\"Accuracy of SVM: {:.2f}%\".format(accuracy_svm * 100))\n",
        "print(\"Accuracy of KNN: {:.2f}%\".format(accuracy_knn * 100))\n"
      ],
      "id": "453fab3f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion\n",
        "\n",
        "In this post, we've explored various classification algorithms, each with its unique strengths. Choosing the right classifier depends on the dataset's size, quality, and nature. Experimentation and cross-validation are key to determining the most effective model for your specific data."
      ],
      "id": "76d0f580"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}